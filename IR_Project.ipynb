{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FTIR Quantification Project**\n",
    "\n",
    "***Plan of Action:***\n",
    "    \n",
    "    1. Import packages\n",
    "\n",
    "    2. Import .csv files w/ Pandas\n",
    "\n",
    "    3. Visualise spectra w/ Matplotlib.pyplot\n",
    "\n",
    "    4. Perform initial exploratory data analysis\n",
    "\n",
    "    5. Normalize the data w/ machine learning (MinMaxScalar)\n",
    "\n",
    "    6. Isolate one data frame, perform the intial steps labelled above (except normilization as this can be imported from the above cells).\n",
    "\n",
    "        6.1 Invert the peaks to allow troughs/peaks to be picked\n",
    "\n",
    "        6.2 Set a threshold for peaks to be picked.\n",
    "\n",
    "        6.3 Perform quantification on these isolated peaks\n",
    "\n",
    "    7. Create a dataframe with the quantification data\n",
    "    \n",
    "    8. Have this dataframe exported to a new file.\n",
    "\n",
    "***Next Steps:***\n",
    "\n",
    "    1. Classify the wavenumbers to correspond to the Epoxy peaks of interest\n",
    "    \n",
    "    2. Understand the variation (if any/ significant) between quantification data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Visualisation packages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Statistical analysis packages\n",
    "from scipy.signal import find_peaks, peak_prominences, peak_widths\n",
    "from scipy.stats import cauchy, ttest_ind\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files into DataFrames\n",
    "# The df's are NOT hardcoded with the exact path location\n",
    "df02 = pd.read_csv('filename')\n",
    "df03 = pd.read_csv('filename')\n",
    "df04 = pd.read_csv('filename')\n",
    "df05 = pd.read_csv('filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib visualisation\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.plot(df02['Wavenumber'], df02['Intensity'], label='02')\n",
    "plt.plot(df03['Wavenumber'], df03['Intensity'], label='03')\n",
    "plt.plot(df04['Wavenumber'], df04['Intensity'], label='04')\n",
    "plt.plot(df05['Wavenumber'], df05['Intensity'], label='05')\n",
    "plt.ylabel('Intensity')\n",
    "plt.title('IR Spectra')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find peaks in spec\n",
    "peaks2, _ = find_peaks(df02['Intensity'], height=0.1)\n",
    "peaks3, _ = find_peaks(df03['Intensity'], height=0.1)\n",
    "peaks4, _ = find_peaks(df04['Intensity'], height=0.1)\n",
    "peaks5, _ = find_peaks(df04['Intensity'], height=0.1)\n",
    "\n",
    "# Quantify changes\n",
    "sum_peaks2 = np.sum(df02['Intensity'][peaks2])\n",
    "sum_peaks3 = np.sum(df03['Intensity'][peaks3])\n",
    "sum_peaks4 = np.sum(df04['Intensity'][peaks4])\n",
    "sum_peaks5 = np.sum(df05['Intensity'][peaks5])\n",
    "\n",
    "# Print changes with concatination\n",
    "print(f'Sum of peak intenisties for df02: {sum_peaks2}')\n",
    "print(f'Sum of peak intenisties for df03: {sum_peaks3}')\n",
    "print(f'Sum of peak intenisties for df04: {sum_peaks4}')\n",
    "print(f'Sum of peak intenisties for df05: {sum_peaks5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normilize the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "df02['Normalized_Intensity'] = scaler.fit_transform(df02[['Intensity']])\n",
    "df03['Normalized_Intensity'] = scaler.fit_transform(df03[['Intensity']])\n",
    "df04['Normalized_Intensity'] = scaler.fit_transform(df04[['Intensity']])\n",
    "df05['Normalized_Intensity'] = scaler.fit_transform(df05[['Intensity']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Understanding**\n",
    "\n",
    "***Depth:*** This is the intensity value at the trough. It indicates how deep the trough is compared to the baseline.\n",
    "\n",
    "***Prominence:*** This measures how much a trough stands out due to its depth and its surrounding peaks. It is the vertical distance between the trough and the highest point of the surrounding baseline.\n",
    "\n",
    "***Width:*** This is the width of the trough at half its depth or FWHM. It provides information about how broad or narrow the trough is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized data\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "# plt.plot(df02['Wavenumber'], df02['Normalized_Intensity'], label='df02 Normalized Intensity')\n",
    "plt.plot(df03['Wavenumber'], df03['Normalized_Intensity'], label='df03 Normalized Intensity')\n",
    "# plt.plot(df04['Wavenumber'], df04['Normalized_Intensity'], label='df04 Normalized Intensity')\n",
    "# plt.plot(df05['Wavenumber'], df05['Normalized_Intensity'], label='df05 Normalized Intensity')\n",
    "\n",
    "# Invert the 'Normalized_Intensity' to find troughs\n",
    "inverted_intensity = -df03['Normalized_Intensity']\n",
    "\n",
    "# Find peaks in the inverted intensity (which are troughs in the original data)\n",
    "potential_troughs, _ = find_peaks(inverted_intensity)\n",
    "\n",
    "# Filter troughs to only include those below the intensity threshold\n",
    "intensity_threshold = 0.1\n",
    "troughs = potential_troughs[df03['Normalized_Intensity'][potential_troughs] < intensity_threshold]\n",
    "\n",
    "# Extract the wavenumber positions of the troughs\n",
    "trough_wavenumbers = df03['Wavenumber'][troughs]\n",
    "\n",
    "# Quantify the troughs\n",
    "depths = df03['Normalized_Intensity'][troughs]  # Depths of the troughs\n",
    "prominences = peak_prominences(inverted_intensity, troughs)[0]  # Prominences of the troughs\n",
    "widths = peak_widths(inverted_intensity, troughs, rel_height=0.5)[0]  # Widths of the troughs\n",
    "\n",
    "# Create a DataFrame to store the quantification results\n",
    "troughs_table = pd.DataFrame({\n",
    "    'Wavenumber': trough_wavenumbers.values,\n",
    "    'Depth': depths.values,\n",
    "    'Prominence': prominences,\n",
    "    'Width': widths\n",
    "})\n",
    "\n",
    "# Print the table\n",
    "print(troughs_table)\n",
    "\n",
    "# Plot the troughs\n",
    "plt.plot(df03['Wavenumber'][troughs], df03['Normalized_Intensity'][troughs], \"x\", label='Troughs in df03')\n",
    "\n",
    "# Annotate the troughs with their wavenumber positions\n",
    "for i, trough in enumerate(troughs):\n",
    "    plt.annotate(f'{df03[\"Wavenumber\"][trough]:.0f}', \n",
    "                (df03['Wavenumber'][trough], df03['Normalized_Intensity'][trough]), \n",
    "                textcoords=\"offset points\", \n",
    "                 xytext=(0,10 + (i % 2) * 10),  # Alternate the offset to avoid overlap\n",
    "                ha='center')\n",
    "\n",
    "plt.xlabel('Wavenumber')\n",
    "plt.ylabel('Normalized Intensity')\n",
    "plt.title('Normalized IR Spectra')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results table to a CSV file\n",
    "troughs_table.to_csv('troughs_quantification_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normalized data\n",
    "plt.figure(figsize=(25, 10))\n",
    "\n",
    "plt.plot(df02['Wavenumber'], df02['Normalized_Intensity'], label='df02 Normalized Intensity')\n",
    "plt.plot(df03['Wavenumber'], df03['Normalized_Intensity'], label='df03 Normalized Intensity')\n",
    "plt.plot(df04['Wavenumber'], df04['Normalized_Intensity'], label='df04 Normalized Intensity')\n",
    "plt.plot(df05['Wavenumber'], df05['Normalized_Intensity'], label='df05 Normalized Intensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Define path to folder containing the CSV files\n",
    "folder_path = r'insert folder path'\n",
    "\n",
    "# Use glob to get all CSV files in the folder\n",
    "csv_files = glob.glob(folder_path)\n",
    "\n",
    "# Loop through the list of CSV files and process each one\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file)\n",
    "        \n",
    "        # Perform your processing here\n",
    "        print(f\"Processing file: {file}\")\n",
    "        print(df.head())  # Example: print the first few rows of each file\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning the path to the folder variable\n",
    "folder = 'insert folder path'\n",
    "\n",
    "#Getting the list of files from the assigned path\n",
    "excel_files = [file for file in os.listdir(folder)]\n",
    "\n",
    "list_of_dfs = []\n",
    "for file in excel_files :\n",
    "    df = pd.concat(pd.read_excel(folder + \"\\\\\" + file, sheet_name=None))\n",
    "    df.index = df.index.get_level_values(0)\n",
    "    df.reset_index().rename({'index':'Tab'}, axis=1)\n",
    "    df['excelfile_name'] = file.split('.')[0]\n",
    "    list_of_dfs.append(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
